{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch ## import pytorch\n",
    "import torch.nn as nn\n",
    "import numpy as np ## import of numpy\n",
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 2, 3)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'ten_n' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5617c35d8a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mten_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## 1D tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ten_n' is not defined"
     ]
    }
   ],
   "source": [
    "## Vector 1D:\n",
    "## Matrix 2D:\n",
    "## Tensor ND: (1-N)\n",
    "## A pytorch tensor (random)\n",
    "vector = np.array([[[1,2,3], [1,2,3]], [[1,2,3], [1,2,3]]])\n",
    "print(vector.shape)\n",
    "\n",
    "\n",
    "print(ten_n.shape)\n",
    "ten = torch.tensor([1., 2., 3.]) ## 1D tensor\n",
    "print(ten.shape)\n",
    "print(ten.type())\n",
    "a = torch.FloatTensor([[[1, 2, 3]]])\n",
    "print(a.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b41661f51435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## clear content of a tensor (alter the tensor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "## clear content of a tensor (without altering the tensor) (Functional, makes a copy\n",
    "## of the tensor)\n",
    "# b = torch.zeros_like(a) ## \n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(a)\n",
    "# print()\n",
    "## clear content of a tensor (alter the tensor)\n",
    "a.zero_()\n",
    "print(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we make a numpy array\n",
    "a_numpy = np.random.normal(loc=0, scale=1.0, size=(3,2)) ## create a random uniform distrib (mean=0, std=1.0 size=(3,2))\n",
    "\n",
    "## convert it into pytorch tensor\n",
    "a_tensor = torch.tensor(a_numpy, dtype=torch.float)\n",
    "#print(a_tensor)\n",
    "a = torch.randn((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.)\ntensor([ 1.,  2., -3.])\ntensor(0.)\ntorch.return_types.mode(\nvalues=tensor(-3.),\nindices=tensor(2))\ntensor(1.)\ntensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "## tensor operations\n",
    "a = torch.tensor([1., 2., -3.])\n",
    "print(a.sum()) ## sum all elements in the tensor\n",
    "print(a)\n",
    "print(a.mean()) ## ## get the mean of the tensor\n",
    "print(a.mode()) ## get the mode of the tensor\n",
    "print(a.median()) ## get the meadian of the tensor\n",
    "print(a.abs()) ## get the abs of the tensor\n",
    "#print(a.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ 1.,  2., -3.])\ncpu\ntensor([ 1.,  2., -3.], device='cuda:0')\ncuda:0\n"
     ]
    }
   ],
   "source": [
    "## GPU tensors\n",
    "print(a) ## this is a cpu tensor when you make it\n",
    "print(a.device) ## to see what the device that tensor assigned to\n",
    "print(a.to('cuda')) ## assign to the GPU\n",
    "print(a.to('cuda').device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nFalse\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nwith torch.no_grad():\\n    ## execute you model\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 219
    }
   ],
   "source": [
    "## tensor gradients\n",
    "v1 = torch.tensor([1.0, 2.0]) ## does not required gradient computation\n",
    "v2 = torch.tensor([3.1, 1.0], requires_grad=True) ## require gradient computation\n",
    "\n",
    "## operate on the tensors\n",
    "v_sum = v1 + v2 ## computation graph step-1\n",
    "v_res = (v_sum*2).sum() ## computation graph step-2\n",
    "#print(v_sum)\n",
    "#print(v_res)\n",
    "\n",
    "## user made tensors are leafs in  a graph\n",
    "## but intermediate operations are not\n",
    "print(v1.is_leaf)\n",
    "print(v_sum.is_leaf)\n",
    "\n",
    "'''\n",
    "with torch.no_grad():\n",
    "    ## execute you model\n",
    "'''\n",
    "## are gradients required\n",
    "#print(v1.requires_grad)\n",
    "#print(v2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2511, -0.0676, -0.0525],\n        [ 0.0820, -0.3997,  0.5529],\n        [ 0.3400,  0.1264,  0.1481],\n        [-0.0356, -0.5256,  0.2352],\n        [ 0.3352, -0.1250,  0.1937]])), ('bias', tensor([-0.3236,  0.3611,  0.1582,  0.1286,  0.3971]))])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "fill_ only supports 0-dimension value tensor but got tensor with 2 dimensions.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-f3f748b42daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlinear_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## A weight matrix and b = biases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: fill_ only supports 0-dimension value tensor but got tensor with 2 dimensions."
     ]
    }
   ],
   "source": [
    "## NN blocks\n",
    "import torch.nn as nn\n",
    "linear_layer = nn.Linear(3, 5, bias=True) ## y = x(A)^T + b\n",
    "print(linear_layer.state_dict())\n",
    "weight = torch.randn((5, 3))\n",
    "linear_layer.weight.data.fill_(weight)\n",
    "\n",
    "## A weight matrix and b = biases\n",
    "inp = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "linear_layer(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<generator object Module.parameters at 0x7f5131541ba0>\nParameter containing:\ntensor([[-0.0636, -0.1714,  0.5334],\n        [-0.4526, -0.0075, -0.0849],\n        [ 0.3653, -0.1952, -0.5339],\n        [ 0.0509, -0.4912, -0.0707],\n        [ 0.2531, -0.4524, -0.2112]], requires_grad=True)\nParameter containing:\ntensor([0.5584, 0.2092, 0.4176, 0.3726, 0.2655], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## layer parameter\n",
    "print(linear_layer.parameters()) ## return a generator that yields the layer parameter (A, b); A: 3x5 b: 3\n",
    "for i in linear_layer.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "OrderedDict([('weight', tensor([[-0.0636, -0.1714,  0.5334],\n        [-0.4526, -0.0075, -0.0849],\n        [ 0.3653, -0.1952, -0.5339],\n        [ 0.0509, -0.4912, -0.0707],\n        [ 0.2531, -0.4524, -0.2112]])), ('bias', tensor([0.5584, 0.2092, 0.4176, 0.3726, 0.2655]))])\n"
     ]
    }
   ],
   "source": [
    "## layer parameter\n",
    "print(linear_layer.state_dict()) ## parameters and biases as a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Dropout(p=0.3, inplace=False)\n",
       "  (7): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 189
    }
   ],
   "source": [
    "## A simple ANN model\n",
    "model = nn.Sequential( ### stack layers to make a neural network graph\n",
    "    nn.Linear(2,5), ## first layer\n",
    "    nn.ReLU(), ## first activation function\n",
    "    nn.Linear(5,20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.3),\n",
    "    nn.Softmax(dim=1) ## Softmax probability\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "inp = torch.FloatTensor([[1,2]]) ## dummy inputs\n",
    "model(inp).shape ## call forward on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## model construction by subclassing\n",
    "# class Model(nn.Module): ## nn.Module the parent class and class Model is the child class\n",
    "#     ## initialize the model class\n",
    "#     def __init__(self, input_shape, output_shape, drop=0.3):\n",
    "#         super(Model, self).__init__()\n",
    "#         '''\n",
    "#         self.model_1 = nn.Sequential(\n",
    "#                         nn.Linear(input_shape,5),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Linear(5,20),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Linear(20,5),\n",
    "#                         )\n",
    "#         self.model_2 = nn.Sequential(\n",
    "#                         nn.Linear(input_shape,5),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Linear(5,20),\n",
    "#                         nn.ReLU(),\n",
    "#                         nn.Linear(20,5),\n",
    "#                         )\n",
    "#         '''\n",
    "#         #self.fc_1 = nn.Linear()\n",
    "#     ## forward pass of the NN\n",
    "#     ## execute the model with the inputs\n",
    "#     def forward(self, x): ## x is the model input\n",
    "#         return F.linear(2, 5)(x)\n",
    "#         #return self.model_1(x) + self.model_2(x)\n",
    "#         #return self.model(x) ## execute and get the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model construction by subclassing\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, drop=0.3):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                        nn.Linear(input_shape,5),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(5,20),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(20,output_shape),\n",
    "                        #nn.ReLU(),\n",
    "                        #nn.Dropout(p=0.3),\n",
    "                        #nn.Softmax(dim=1)\n",
    "                        )\n",
    "    ## forward pass of the NN\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 2])\ntorch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "## construct the model\n",
    "net = Model(2, 1) ## contruct the model object\n",
    "inp = torch.FloatTensor([[2, 3]]) ## 1x2\n",
    "print(inp.shape)\n",
    "out = net(inp) ## call the model\n",
    "#print(net)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1000, 2])\n"
     ]
    }
   ],
   "source": [
    "## make some random data\n",
    "inp_batch = torch.normal(0, 1, (1000, 2)) ## 1000x input_size training data ## (1000, c, h, w)\n",
    "print(inp_batch.size())\n",
    "tgt_batch = torch.normal(0, 1, (1000, 1)) # 1000x output_size training data ## (1000, cat or dog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "79662037\n",
      "0.93567088291049\n",
      "0.9356694009900093\n",
      "0.9357641416788102\n",
      "0.9356085073947906\n",
      "0.9356935553252697\n",
      "0.9356047382950783\n",
      "0.9357109233736992\n",
      "0.9357283003628254\n",
      "0.9356097076833249\n",
      "0.9356378895044327\n",
      "0.9356434515118599\n",
      "0.9357337358593941\n",
      "0.9356496964395046\n",
      "0.9355632607638836\n",
      "0.9357718655467033\n",
      "0.9355629953742027\n",
      "0.9356202192604541\n",
      "0.9356119003891945\n",
      "0.9356359621882439\n",
      "0.9356990000605583\n",
      "0.9355771526694298\n",
      "0.9356330972909928\n",
      "0.9356820087134838\n",
      "0.9355535957217217\n",
      "0.9355944633483887\n",
      "0.9356013496220111\n",
      "0.935600993335247\n",
      "0.9355988848209381\n",
      "0.9355808751285076\n",
      "0.9356887006759643\n",
      "0.9355555316805839\n",
      "0.9355433973670005\n",
      "0.9356751069426537\n",
      "0.9355405279994011\n",
      "0.9355336447060109\n",
      "0.9355956986546516\n",
      "0.9356931227445603\n",
      "0.9355186213552952\n",
      "0.9355014596879482\n",
      "0.9355756866931916\n",
      "0.9355295392870903\n",
      "0.9356514038145543\n",
      "0.9355037353932858\n",
      "0.9355208170413971\n",
      "0.9356468482315541\n",
      "0.9355174168944359\n",
      "0.9356091682612896\n",
      "0.935480551123619\n",
      "0.9355504953861237\n",
      "0.9354934914410115\n",
      "0.9356366369128227\n",
      "0.9354556514322758\n",
      "0.9356371971964836\n",
      "0.9354638290405274\n",
      "0.9355186131596566\n",
      "0.9355167712271214\n",
      "0.935463070422411\n",
      "0.935496452152729\n",
      "0.9354758174717426\n",
      "0.9356253516674041\n",
      "0.9354349461197853\n",
      "0.9354777817428112\n",
      "0.9354560974240304\n",
      "0.9356076078116894\n",
      "0.9354385854303837\n",
      "0.935589781999588\n",
      "0.9354218664765358\n",
      "0.9355558940768242\n",
      "0.935448010712862\n",
      "0.9354321156442166\n",
      "0.935479679107666\n",
      "0.9354121728241444\n",
      "0.9355829751491547\n",
      "0.9354197198152542\n",
      "0.9353928957879544\n",
      "0.9354816903173924\n",
      "0.935418793708086\n",
      "0.9354698778688908\n",
      "0.9355078415572643\n",
      "0.9354121001064777\n",
      "0.9355305920541287\n",
      "0.9353787288069725\n",
      "0.9354309935867786\n",
      "0.9353832674026489\n",
      "0.935530127286911\n",
      "0.9353589089214802\n",
      "0.935521955192089\n",
      "0.9353684559464455\n",
      "0.9353877909481525\n",
      "0.9354603593051434\n",
      "0.9353684602677822\n",
      "0.935394050180912\n",
      "0.9355146600306035\n",
      "0.9353579922020435\n",
      "0.9354416798055172\n",
      "0.9353786218166351\n",
      "0.9354733127355576\n",
      "0.9353651800751686\n",
      "0.9353306919336319\n",
      "0.9354010319709778\n",
      "0.93549506559968\n",
      "0.9353148797154427\n",
      "0.9353247326612473\n",
      "0.9355305883288384\n",
      "0.9353679227828979\n",
      "0.9352887193858623\n",
      "0.935371091067791\n",
      "0.9353355321288109\n",
      "0.935369208753109\n",
      "0.9353309553861618\n",
      "0.9354430955648422\n",
      "0.9352930454909801\n",
      "0.9353322830796241\n",
      "0.935332915186882\n",
      "0.9353681156039237\n",
      "0.935310260951519\n",
      "0.9354687683284283\n",
      "0.9352539047598839\n",
      "0.935358792245388\n",
      "0.935389362424612\n",
      "0.9352812175452709\n",
      "0.9353250713646412\n",
      "0.9353156052529812\n",
      "0.9353114497661591\n",
      "0.9352579513192176\n",
      "0.9353079989552497\n",
      "0.9353115828335286\n",
      "0.9354225713014602\n",
      "0.9352181416749954\n",
      "0.9352718818187714\n",
      "0.9352779512107372\n",
      "0.9354210992157459\n",
      "0.9352332226932049\n",
      "0.9352859967947006\n",
      "0.9353742626309395\n",
      "0.9351913730800152\n",
      "0.9352673937380314\n",
      "0.9352735579013824\n",
      "0.9352640725672245\n",
      "0.935373623073101\n",
      "0.9351666808128357\n",
      "0.9352461540699005\n",
      "0.935289836525917\n",
      "0.9353783637285232\n",
      "0.9351755391061306\n",
      "0.9352271538972855\n",
      "0.9351815450191497\n",
      "0.9353517070412636\n",
      "0.9352322971820831\n",
      "0.935232230424881\n",
      "0.9352387677133084\n",
      "0.9353135602176189\n",
      "0.9351843383908272\n",
      "0.9351581341028213\n",
      "0.935208231806755\n",
      "0.9352174991369248\n",
      "0.9351815064251423\n",
      "0.9352168717980385\n",
      "0.9353119727969169\n",
      "0.9351548838615418\n",
      "0.9353105586767196\n",
      "0.9351579163968563\n",
      "0.9351951730251312\n",
      "0.9352747924625874\n",
      "0.9351470643281936\n",
      "0.9352709072828292\n",
      "0.935135228484869\n",
      "0.9351556803286075\n",
      "0.9352206109464168\n",
      "0.9350941900908947\n",
      "0.9353157480061054\n",
      "0.9351147402822971\n",
      "0.9351232823729515\n",
      "0.9352935567498207\n",
      "0.9350964616239071\n",
      "0.9351480576395989\n",
      "0.9351904618740082\n",
      "0.9352398900687695\n",
      "0.9351088158786297\n",
      "0.9351542635262012\n",
      "0.9352569925785065\n",
      "0.9350440287590027\n",
      "0.9351253175735473\n",
      "0.9350862793624402\n",
      "0.9351532688736915\n",
      "0.9351821522414684\n",
      "0.9351325978338718\n",
      "0.9351056300103664\n",
      "0.9350504964590073\n",
      "0.9352676829695702\n",
      "0.9350791370868683\n",
      "0.9350985518097877\n",
      "0.9350618952512741\n",
      "0.9351066388189793\n",
      "0.9351901316642761\n",
      "0.9350688110291958\n",
      "0.9351127961277962\n",
      "0.9351639702916146\n",
      "0.9350514245033265\n",
      "0.9350609816610813\n",
      "0.9350854510068893\n",
      "0.9352027672529221\n",
      "0.9350411185622215\n",
      "0.9350888049602508\n",
      "0.9350357101857663\n",
      "0.9351575562357902\n",
      "0.9350635260343552\n",
      "0.9350364166498184\n",
      "0.9350389063358306\n",
      "0.9351780042052269\n",
      "0.9349894186854363\n",
      "0.9350437301397324\n",
      "0.9351384703814983\n",
      "0.9349767781794072\n",
      "0.9351471009850502\n",
      "0.9350106734037399\n",
      "0.9349846404790878\n",
      "0.935175863802433\n",
      "0.9349954645335674\n",
      "0.9349646186828613\n",
      "0.9350591990351677\n",
      "0.9351420474052429\n",
      "0.9349748779833317\n",
      "0.935033111423254\n",
      "0.9349832701683044\n",
      "0.9350088003277779\n",
      "0.9350545713305474\n",
      "0.9349827936291695\n",
      "0.9349609780311584\n",
      "0.9351032297313213\n",
      "0.9350052610039711\n",
      "0.9349732938408851\n",
      "0.9350733353197574\n",
      "0.934964505136013\n",
      "0.9349303343892097\n",
      "0.9349771727621555\n",
      "0.9350718769431114\n",
      "0.9349050123989582\n",
      "0.9349836872518063\n",
      "0.9349509823322296\n",
      "0.9350484488904476\n",
      "0.9349202515184879\n",
      "0.9349681976437568\n",
      "0.9348901578783989\n",
      "0.9350947615504265\n",
      "0.934885164052248\n",
      "0.9350465278327466\n",
      "0.9349233467876911\n",
      "0.9350119490921497\n",
      "0.9349022218585015\n",
      "0.9349237343668938\n",
      "0.935037718564272\n",
      "0.9349030148983002\n",
      "0.9348644953966141\n",
      "0.9349576672911644\n",
      "0.9348831430077553\n",
      "0.9350295192003251\n",
      "0.9348458060622216\n",
      "0.9350144833326339\n",
      "0.934874572455883\n",
      "0.934908413887024\n",
      "0.9348318836092949\n",
      "0.9350334015488625\n",
      "0.9348693868517876\n",
      "0.9349269941449165\n",
      "0.9348228108882904\n",
      "0.9349239024519921\n",
      "0.9349563397467137\n",
      "0.9348523792624474\n",
      "0.9348150910437107\n",
      "0.9349801057577133\n",
      "0.9348275887966156\n",
      "0.9348662367463112\n",
      "0.9348815743625164\n",
      "0.9349342615902424\n",
      "0.934844262599945\n",
      "0.9348859505355358\n",
      "0.9348467317223549\n",
      "0.9348132561147213\n",
      "0.9348326778411865\n",
      "0.9348383986949921\n",
      "0.9349397176504135\n",
      "0.9348385363817215\n",
      "0.9347809679806233\n",
      "0.934943143427372\n",
      "0.9347891731560231\n",
      "0.9348988643288613\n",
      "0.9347763796150684\n",
      "0.9348057296872139\n",
      "0.9347774589061737\n",
      "0.9349480703473091\n",
      "0.93478296905756\n",
      "0.934803404211998\n",
      "0.9348609945178032\n",
      "0.9347674150764942\n",
      "0.9348159311711788\n",
      "0.9348740327358246\n",
      "0.9347866508364677\n",
      "0.9347351896762848\n",
      "0.9348579117655754\n",
      "0.9347854879498482\n",
      "0.9347585341334343\n",
      "0.9348342572152615\n",
      "0.9347652071714401\n",
      "0.9347554522752762\n",
      "0.9348308555781841\n",
      "0.9347153395414353\n",
      "0.9347771306335926\n",
      "0.9347019523382187\n",
      "0.9348602111637593\n",
      "0.934734582901001\n",
      "0.9347413778305054\n",
      "0.9347137345373631\n",
      "0.9348689121007919\n",
      "0.9346928274631501\n",
      "0.934693603515625\n",
      "0.9348184543848038\n",
      "0.934714998304844\n",
      "0.9346184861660004\n",
      "0.9348705558478833\n",
      "0.9346869099140167\n",
      "0.934662610590458\n",
      "0.9347638636827469\n",
      "0.9346803297102452\n",
      "0.9348080083727837\n",
      "0.9346458351612091\n",
      "0.9347930173575878\n",
      "0.9346402512490749\n",
      "0.9347722235321999\n",
      "0.934672790914774\n",
      "0.9346728603541851\n",
      "0.9347256615757942\n",
      "0.9346453978121281\n",
      "0.9347972777485848\n",
      "0.934617110490799\n",
      "0.9346438029408455\n",
      "0.9346777833998203\n",
      "0.9347646969556809\n",
      "0.9346130341291428\n",
      "0.9346911312639713\n",
      "0.9347271838784218\n",
      "0.9346276238560677\n",
      "0.9346144707500934\n",
      "0.9346512328088283\n",
      "0.934628691971302\n",
      "0.9347513318061829\n",
      "0.9345578081905842\n",
      "0.934730748385191\n",
      "0.9346001130342484\n",
      "0.9345653864741326\n",
      "0.9346453854441643\n",
      "0.9346949574351311\n",
      "0.9345685677230358\n",
      "0.9346883770823479\n",
      "0.9345698703825474\n",
      "0.9345685312151909\n",
      "0.9346771702170372\n",
      "0.9345522464811802\n",
      "0.9345366629958153\n",
      "0.934717290699482\n",
      "0.9345380762219428\n",
      "0.9345291501283646\n",
      "0.9346631956100464\n",
      "0.9345588929951191\n",
      "0.934625370502472\n",
      "0.9344749841094017\n",
      "0.9346345616877079\n",
      "0.9345054784417153\n",
      "0.934530647546053\n",
      "0.9344930490851402\n",
      "0.9345310521125794\n",
      "0.9345404461026192\n",
      "0.9346137729287147\n",
      "0.9344976830482483\n",
      "0.9345852226018906\n",
      "0.9344499716162682\n",
      "0.9345723204314709\n",
      "0.9344047747552395\n",
      "0.9346380341053009\n",
      "0.9344239631295204\n",
      "0.9344927361607551\n",
      "0.9344617821276188\n",
      "0.9344868171215057\n",
      "0.9345435020327568\n",
      "0.9344493941962719\n",
      "0.9345518864691258\n",
      "0.9344411623477936\n",
      "0.9344728679955006\n",
      "0.9345059058070183\n",
      "0.9344433395564556\n",
      "0.9344220058619976\n",
      "0.9344274979829789\n",
      "0.934471505433321\n",
      "0.9345249643921852\n",
      "0.9344180700182915\n",
      "0.9345141917467117\n",
      "0.9344133153557778\n",
      "0.9344562104344368\n",
      "0.9344181440770626\n",
      "0.9343967120349407\n",
      "0.9345131336152553\n",
      "0.9343814586102962\n",
      "0.9344196462631226\n",
      "0.9345058065652847\n",
      "0.9343394082784653\n",
      "0.9344915932416916\n",
      "0.9343709272146224\n",
      "0.9343585814535618\n",
      "0.9344234719872475\n",
      "0.9344124771654606\n",
      "0.9344425602257251\n",
      "0.9343545958399773\n",
      "0.934355646520853\n",
      "0.9344524559378624\n",
      "0.9344295240938664\n",
      "0.9343186102807521\n",
      "0.9343409290909768\n",
      "0.934380544424057\n",
      "0.9344491425156594\n",
      "0.9343326738476754\n",
      "0.9344165921211243\n",
      "0.9343059507012367\n",
      "0.9342956678569316\n",
      "0.9344247755408287\n",
      "0.9343132054805756\n",
      "0.9343165688216686\n",
      "0.9344700336456299\n",
      "0.9342822948098183\n",
      "0.9342887398600578\n",
      "0.9344036234915256\n",
      "0.9342867708206177\n",
      "0.9344094660878182\n",
      "0.9342480990290641\n",
      "0.9343026384711266\n",
      "0.934387916624546\n",
      "0.9342845909297466\n",
      "0.9342583279311657\n",
      "0.9343436011672019\n",
      "0.934253598600626\n",
      "0.9342738293111325\n",
      "0.9343140521645545\n",
      "0.9342304369807244\n",
      "0.9342297926545143\n",
      "0.9343173705041409\n",
      "0.9342475336790085\n",
      "0.9341951322555542\n",
      "0.9343090519309044\n",
      "0.9342388965189456\n",
      "0.934209578037262\n",
      "0.9342744451761246\n",
      "0.93415476962924\n",
      "0.9343052941560746\n",
      "0.9341691528260708\n",
      "0.9342270116508007\n",
      "0.9341884109377862\n",
      "0.9341789285838604\n",
      "0.9341686494648457\n",
      "0.9341546334326267\n",
      "0.934249659627676\n",
      "0.9341691175103187\n",
      "0.9342011454701423\n",
      "0.9341069725155831\n",
      "0.9341417242586613\n",
      "0.9341320860385894\n",
      "0.9342633354663848\n",
      "0.934100528806448\n",
      "0.9341288135945797\n",
      "0.9341119436919689\n",
      "0.9342272107303142\n",
      "0.9340890435874463\n",
      "0.9342007228732109\n",
      "0.9340715791285038\n",
      "0.9340386489033699\n",
      "0.9342014536261558\n",
      "0.934012522995472\n",
      "0.9340774101018906\n",
      "0.9341407994925975\n",
      "0.9340369907021523\n",
      "0.9341438540816307\n",
      "0.9340430417656899\n",
      "0.934036814570427\n",
      "0.9340512910485268\n",
      "0.9341379536688328\n",
      "0.9341039928793907\n",
      "0.933989654481411\n",
      "0.9340017332136631\n",
      "0.9340265187621116\n",
      "0.9340340849757195\n",
      "0.9340830996632576\n",
      "0.9340748086571693\n",
      "0.933942691385746\n",
      "0.9340873311460018\n",
      "0.9339741770923138\n",
      "0.9339596271514893\n",
      "0.9340848064422608\n",
      "0.9339368407428265\n",
      "0.9339275974035263\n",
      "0.9339693987369537\n",
      "0.934019023925066\n",
      "0.9339050026237965\n",
      "0.9340237180888653\n",
      "0.9338653279840946\n",
      "0.9340299780666829\n",
      "0.9339009559154511\n",
      "0.9338907887041569\n",
      "0.9339921399950981\n",
      "0.9339140827953816\n",
      "0.9338666677474976\n",
      "0.9339009702205658\n",
      "0.9339582642912865\n",
      "0.9338509941101074\n",
      "0.9339593417942524\n",
      "0.9338087283074856\n",
      "0.93397625207901\n",
      "0.9338198731839656\n",
      "0.9338491812348366\n",
      "0.9339594593644143\n",
      "0.93381721585989\n",
      "0.9338732205331326\n",
      "0.9339396467804909\n",
      "0.9338416287302971\n",
      "0.933921025544405\n",
      "0.9337945683300495\n",
      "0.9338173089921474\n",
      "0.9338966932892799\n",
      "0.9338120397925377\n",
      "0.9338090263307095\n",
      "0.9338213215768337\n",
      "0.9339581236243248\n",
      "0.9337591713666916\n",
      "0.9338668026030064\n",
      "0.933759231865406\n",
      "0.9337639701366425\n",
      "0.9339189487695694\n",
      "0.9337351141870022\n",
      "0.9337501007318497\n",
      "0.9339107543230056\n",
      "0.9337556031346321\n",
      "0.9337348854541778\n",
      "0.9338707622885704\n",
      "0.933720025420189\n",
      "0.9337841230630874\n",
      "0.933740561157465\n",
      "0.9338443517684937\n",
      "0.9337118983268737\n",
      "0.9337462258338928\n",
      "0.933744882196188\n",
      "0.933738625049591\n",
      "0.9338326062262058\n",
      "0.9337981110811233\n",
      "0.9336210097372531\n",
      "0.9337337014079093\n",
      "0.9337018126249313\n",
      "0.9337962810695172\n",
      "0.9337106317281723\n",
      "0.9336483038961887\n",
      "0.9337397660315037\n",
      "0.9337916323542594\n",
      "0.9336559915542603\n",
      "0.9337715418636798\n",
      "0.9336314640939236\n",
      "0.9337243194878101\n",
      "0.9336228029429913\n",
      "0.9337979657948017\n",
      "0.9336051341891288\n",
      "0.9337642341852188\n",
      "0.9336575332283974\n",
      "0.9337555605173111\n",
      "0.9336190244555473\n",
      "0.9336224365234375\n",
      "0.933665298819542\n",
      "0.9336220039427281\n",
      "0.9337287352979183\n",
      "0.9336190682649612\n",
      "0.9336071190237999\n",
      "0.9337347488105298\n",
      "0.9336286020278931\n",
      "0.933583879172802\n",
      "0.9336779943108559\n",
      "0.9335966087877751\n",
      "0.933720157891512\n",
      "0.9336206696927547\n",
      "0.9335895523428916\n",
      "0.9335900804400444\n",
      "0.9337193112075329\n",
      "0.9335552850365638\n",
      "0.9335427372157574\n",
      "0.9337194287776946\n",
      "0.933543114066124\n",
      "0.9335549676418304\n",
      "0.933602152466774\n",
      "0.9335865667462349\n",
      "0.9336712312698364\n",
      "0.9335169118642807\n",
      "0.9335446375608444\n",
      "0.9336421398818493\n",
      "0.9335104407370091\n",
      "0.9335549081861972\n",
      "0.9335456794500351\n",
      "0.9336637969315053\n",
      "0.9334890662133694\n",
      "0.9335991607606411\n",
      "0.9335385829210281\n",
      "0.9335198868811131\n",
      "0.9335240317881107\n",
      "0.9335939452052117\n",
      "0.9335151201486588\n",
      "0.9334567615389824\n",
      "0.9336479733884334\n",
      "0.933474279642105\n",
      "0.9335374803841114\n",
      "0.9335804361104966\n",
      "0.9334202700853348\n",
      "0.9335874059796333\n",
      "0.9334475031495094\n",
      "0.9334470711648464\n",
      "0.9336228623986245\n",
      "0.9334536334872245\n",
      "0.933466669768095\n",
      "0.9334622301161289\n",
      "0.9335558703541755\n",
      "0.9334707412123681\n",
      "0.9334586940705776\n",
      "0.9335318641364574\n",
      "0.9333501549065113\n",
      "0.9335688008368015\n",
      "0.9334060677886009\n",
      "0.9334213498234749\n",
      "0.9335705384612083\n",
      "0.9333806873857975\n",
      "0.9334468406438827\n",
      "0.9335400423407555\n",
      "0.9334149265289307\n",
      "0.9333590830862523\n",
      "0.9334380929172039\n",
      "0.9335124179720878\n",
      "0.9333866462111473\n",
      "0.9335263808071613\n",
      "0.9333576549589634\n",
      "0.9333848905563354\n",
      "0.9335204634070396\n",
      "0.9333722272515297\n",
      "0.9333282306790351\n",
      "0.9334522078931332\n",
      "0.9335164931416512\n",
      "0.9333382771909237\n",
      "0.9333756031095981\n",
      "0.9333641158044338\n",
      "0.9334638154506684\n",
      "0.9333616264164448\n",
      "0.9334993481636047\n",
      "0.9333086481690407\n",
      "0.9333353246748447\n",
      "0.9334707798063755\n",
      "0.9333024011552333\n",
      "0.9334430500864983\n",
      "0.933280725479126\n",
      "0.9333381743729114\n",
      "0.9334391236305237\n",
      "0.9333075997233391\n",
      "0.9333283278346062\n",
      "0.9332972133159637\n",
      "0.9334368813037872\n",
      "0.9332489202916622\n",
      "0.9333232413232326\n",
      "0.9333993953466415\n",
      "0.9332643657922745\n",
      "0.9334357890486717\n",
      "0.9332402917742729\n",
      "0.9333080098032951\n",
      "0.9333931788802147\n",
      "0.9332831262052059\n",
      "0.9332272174954415\n",
      "0.9333956168591976\n",
      "0.9332628309726715\n",
      "0.9332170259952545\n",
      "0.9333991169929504\n",
      "0.9332442942261696\n",
      "0.9333660726249218\n",
      "0.933215656876564\n",
      "0.9332150867581368\n",
      "0.9333990688621998\n",
      "0.9331729607284069\n",
      "0.933235482275486\n",
      "0.9333405423164368\n",
      "0.9331814350187778\n",
      "0.9332484930753708\n",
      "0.9332306431233883\n",
      "0.9332035122811795\n",
      "0.933351948261261\n",
      "0.9331405514478683\n",
      "0.9333131438493729\n",
      "0.9331883257627487\n",
      "0.9333031517267227\n",
      "0.9331927527487278\n",
      "0.93318956553936\n",
      "0.933273606300354\n",
      "0.9331479725241661\n",
      "0.9331596975028514\n",
      "0.9332276411354542\n",
      "0.933211420327425\n",
      "0.9331085276603699\n",
      "0.9332524970173836\n",
      "0.9332747331261635\n",
      "0.9331631071865558\n",
      "0.9331040382385254\n",
      "0.933246243596077\n",
      "0.93309864833951\n",
      "0.9332268381118775\n",
      "0.9330601172149181\n",
      "0.9331773145496846\n",
      "0.9332657238841057\n",
      "0.9330762648582458\n",
      "0.9331111809611321\n",
      "0.933232504427433\n",
      "0.9330872498452664\n",
      "0.933092228025198\n",
      "0.9331552037596702\n",
      "0.9331956630945206\n",
      "0.9331010562181473\n",
      "0.9332611434161663\n",
      "0.9330166679620743\n",
      "0.9330142931640149\n",
      "0.9331144258379936\n",
      "0.9331364864110947\n",
      "0.9331888380646706\n",
      "0.9330310159921646\n",
      "0.9332114550471305\n",
      "0.9330238375067711\n",
      "0.9330472972989082\n",
      "0.9331053367257118\n",
      "0.9331992544233799\n",
      "0.9329854661226272\n",
      "0.9332025933265686\n",
      "0.9330363315343857\n",
      "0.933175151348114\n",
      "0.9330111281573772\n",
      "0.9330041646957398\n",
      "0.9331819225847721\n",
      "0.9330519324541092\n",
      "0.9330287024378776\n",
      "0.9331475922465324\n",
      "0.9330224794149399\n",
      "0.9330826936662198\n",
      "0.9331243965029716\n",
      "0.9330179235339164\n",
      "0.933002512305975\n",
      "0.9331503662467003\n",
      "0.9330288107693195\n",
      "0.9330144447088241\n",
      "0.9331173427402973\n",
      "0.9330085061490536\n",
      "0.933048682063818\n",
      "0.9330005139112473\n",
      "0.9330145615339279\n",
      "0.9331072872877121\n",
      "0.932925073504448\n",
      "0.9330392397940159\n",
      "0.9330376975238324\n",
      "0.9329599466919899\n",
      "0.9329813554883003\n",
      "0.9331201387941838\n",
      "0.9329000622034073\n",
      "0.933091901987791\n",
      "0.9328905466198921\n",
      "0.9331049005687236\n",
      "0.9329084229469299\n",
      "0.9329614967107773\n",
      "0.932954416424036\n",
      "0.9330926358699798\n",
      "0.9328917774558068\n",
      "0.9330876199901104\n",
      "0.9329265271127224\n",
      "0.9330077649652958\n",
      "0.9329274608194829\n",
      "0.932978387773037\n",
      "0.9330762670934201\n",
      "0.932875247746706\n",
      "0.9329333972930908\n",
      "0.9330264531075955\n",
      "0.9328719300031661\n",
      "0.9330681496858597\n",
      "0.9328759768605233\n",
      "0.9329620243608951\n",
      "0.9330110749602318\n",
      "0.9328484462201595\n",
      "0.9330065175890923\n",
      "0.9328485463559627\n",
      "0.9328616979718208\n",
      "0.932928663045168\n",
      "0.9330296421051025\n",
      "0.9328460240364075\n",
      "0.9329974386096\n",
      "0.9328467234969139\n",
      "0.9330101473629475\n",
      "0.9328086461126804\n",
      "0.9328382790088654\n",
      "0.9329929867386818\n",
      "0.9328603345155716\n",
      "0.9328958211839199\n",
      "0.9328796049952507\n",
      "0.9330118651688099\n",
      "0.9327823893725872\n",
      "0.9329011386632919\n",
      "0.9329584610462188\n",
      "0.9328230917453766\n",
      "0.9329516449570656\n",
      "0.9328714525699615\n",
      "0.9328238627314568\n",
      "0.9329731756448746\n",
      "0.9328059579432011\n",
      "0.9328596355020999\n",
      "0.9330128625035286\n",
      "0.9327504834532738\n",
      "0.932990081012249\n",
      "0.9327929708361625\n",
      "0.9329656106233597\n",
      "0.9327503460645675\n",
      "0.9329210744798183\n",
      "0.9328217375278473\n",
      "0.932818610072136\n",
      "0.9329171253740788\n",
      "0.9327509844303131\n",
      "0.932839715629816\n",
      "0.9329557113349438\n",
      "0.9327454626560211\n",
      "0.9329450176656247\n",
      "0.9327129773795605\n",
      "0.932828899025917\n",
      "0.9329057306051254\n",
      "0.9327380268275738\n",
      "0.9329338203370571\n",
      "0.93279685780406\n",
      "0.9327782317996025\n",
      "0.9329273217916488\n",
      "0.9327672970294952\n",
      "0.932881355881691\n",
      "0.9327350698411465\n",
      "0.9328037026524544\n",
      "0.9328974013030529\n",
      "0.9327126242220402\n",
      "0.93289580732584\n",
      "0.9327502512931823\n",
      "0.9328278687596321\n",
      "0.9327875919640064\n",
      "0.9327515450119972\n",
      "0.9328862270712852\n",
      "0.9328426550328731\n",
      "0.9326711231470108\n",
      "0.9327648702263832\n",
      "0.932751233279705\n",
      "0.932859708070755\n",
      "0.932742854654789\n",
      "0.9327559371292591\n",
      "0.9328444196283817\n",
      "0.9326959981024265\n",
      "0.9328316202759743\n",
      "0.9327040985226631\n",
      "0.9328550325334072\n",
      "0.9326745116710663\n",
      "0.9327200956642627\n",
      "0.9328317853808403\n",
      "0.9326825544238091\n",
      "0.9328072924911975\n",
      "0.9326512885093688\n",
      "0.9326466737687588\n",
      "0.9328165066242218\n",
      "0.9326414406299591\n",
      "0.9327120414376259\n",
      "0.9327778778970242\n",
      "0.932631290704012\n",
      "0.9328116005659104\n",
      "0.9325832283496857\n",
      "0.9327958476543426\n",
      "0.9326001906394958\n",
      "0.9327579778432846\n",
      "0.9325842030346394\n",
      "0.9326410892605782\n",
      "0.9327641941606999\n",
      "0.9325827451050281\n",
      "0.9326104217767716\n",
      "0.932751462906599\n",
      "0.9326248766481876\n",
      "0.9327078707516193\n",
      "0.9325675225257873\n",
      "0.9326229849457741\n",
      "0.9326983796060085\n",
      "0.9325462158024311\n",
      "0.9327174459397792\n",
      "0.9324974942207337\n",
      "0.9327770042419433\n",
      "0.9324952368438244\n",
      "0.9327072793245316\n",
      "0.9325058558583259\n",
      "0.9325818434357643\n",
      "0.93269058406353\n",
      "0.9326550745964051\n",
      "0.9324639931321144\n",
      "0.9325484296679497\n",
      "0.9326817779242993\n",
      "0.9324380666017532\n",
      "0.9325383351743222\n",
      "0.9326700620353222\n",
      "0.9325004820525646\n",
      "0.9326697508990764\n",
      "0.9325586821138859\n",
      "0.9326174035668373\n",
      "0.9324499621987343\n",
      "0.9325077264010906\n",
      "0.932599826157093\n",
      "0.9324719716608524\n",
      "0.9325101223587989\n",
      "0.9326145136356354\n",
      "0.9324155348539352\n",
      "0.93262086763978\n",
      "0.9324097315967083\n",
      "0.9325840665400028\n",
      "0.9324366298317909\n",
      "0.9325889420509338\n",
      "0.9324033617973327\n",
      "0.9325587931275368\n",
      "0.9324487952888012\n",
      "0.9325441238284111\n",
      "0.9324471665918828\n",
      "0.932427738904953\n",
      "0.9325487975776195\n",
      "0.9325371086597443\n",
      "0.9323888576030731\n",
      "0.9325414811074734\n",
      "0.9323521772027016\n",
      "0.9325140546262264\n",
      "0.9323286055028439\n",
      "0.9325396025180817\n",
      "0.9324032026529312\n",
      "0.93233921661973\n",
      "0.9324227869510651\n",
      "0.9325197196006775\n",
      "0.9324451945722103\n",
      "0.9323274809122085\n",
      "0.9325006167590618\n",
      "0.9323395279049873\n",
      "0.9325225150585175\n",
      "0.9322874496877194\n",
      "0.9323634813725948\n",
      "0.9324626679718494\n",
      "0.9323123288154602\n",
      "0.9324545538425446\n",
      "0.932287125736475\n",
      "0.9323675443232059\n",
      "0.9324767746031284\n",
      "0.9322887332737446\n",
      "0.9324570761620998\n",
      "0.9322535495460034\n",
      "0.9324397440254688\n",
      "0.9322506621479988\n",
      "0.9324341171979904\n",
      "0.9322871373593807\n",
      "0.932441277205944\n",
      "0.9322108274698258\n",
      "0.9324089640378952\n",
      "0.9322512990236282\n",
      "0.9324471320211888\n",
      "0.9323758605122566\n",
      "0.9321661727130413\n",
      "0.9324042701721191\n",
      "0.9322411495447159\n",
      "0.9322775858640671\n",
      "0.9323916992545128\n",
      "0.9322289085388183\n",
      "0.9323919090628624\n",
      "0.932345809340477\n",
      "0.9322209891676903\n",
      "0.932347923964262\n",
      "0.9322480610013009\n",
      "0.9323573477566243\n",
      "0.932168780118227\n",
      "0.932382536381483\n",
      "0.9321425351500511\n",
      "0.9323793804645538\n",
      "0.9322038032114506\n",
      "0.9322296963632106\n",
      "0.9323568305373192\n",
      "0.9322935397922992\n",
      "0.9321828651428222\n",
      "0.9323155862092972\n",
      "0.9321207115054131\n",
      "0.9323615746200085\n",
      "0.9321756215393543\n",
      "0.9323604029417037\n",
      "0.9321722568571568\n",
      "0.932300032377243\n",
      "0.9321349214017391\n",
      "0.932288339883089\n",
      "0.9321812696754932\n",
      "0.932277422696352\n",
      "0.9321395573019982\n",
      "0.9321716092526913\n",
      "0.9323316004872322\n",
      "0.932086933106184\n",
      "0.9322826670110226\n",
      "0.9322283880412578\n",
      "0.9320780888199807\n",
      "0.9322577008605003\n",
      "0.9321364435553551\n",
      "0.9322629687190056\n",
      "0.932083523273468\n",
      "0.9322736185789108\n",
      "0.9320469358563424\n",
      "0.9323067793250084\n",
      "0.932057646214962\n",
      "0.9322040392458439\n",
      "0.932079232186079\n",
      "0.9322605940699578\n",
      "0.9320557731389999\n",
      "0.9322399513423443\n",
      "0.9320685729384423\n",
      "0.9321742157638073\n",
      "0.9320950649678708\n",
      "0.9322232599556446\n",
      "0.9320417377352714\n",
      "0.9321525901556015\n",
      "0.9321229949593544\n",
      "0.9321550963819027\n",
      "0.9321197259426117\n",
      "0.9321882559359074\n",
      "0.9320676520466804\n",
      "0.932147059738636\n",
      "0.9320380333065986\n",
      "0.9322050039470196\n",
      "0.9320434150099755\n",
      "0.9321584232151509\n",
      "0.9320157673954964\n",
      "0.9321724699437618\n",
      "0.9319938023388386\n",
      "0.9321683007478714\n",
      "0.9321138262748718\n",
      "0.9320152927935124\n",
      "0.9321971255540847\n",
      "0.9320301100611686\n",
      "0.9321553860604763\n",
      "0.9319791276752949\n",
      "0.9321492980420589\n",
      "0.9319688814878464\n",
      "0.932127984315157\n",
      "0.931996383368969\n",
      "0.932107182443142\n",
      "0.9319886037707329\n",
      "0.9321687698364258\n",
      "0.9319777145981789\n",
      "0.9320997673273087\n",
      "0.9319498163461685\n",
      "0.93208940833807\n",
      "0.9319694055616856\n",
      "0.9320792385935783\n",
      "0.9321073818206788\n",
      "0.93193055793643\n",
      "0.9321092286705971\n",
      "0.9319030310213566\n",
      "0.9320923864841462\n",
      "0.9319199945032597\n"
     ]
    }
   ],
   "source": [
    "#writer = SummaryWriter()\n",
    "for e in range(10000): ## \n",
    "    epoch_loss = list()\n",
    "    for i in range(0, inp_batch.shape[0], 10):\n",
    "        inp = inp_batch[i:i+10] ## get the batches of data (10x2), (10x2)\n",
    "        tgt = tgt_batch[i:i+10]## get the batches of data (10x1), (10x1)\n",
    "        out = net(inp) ## call the model with the input batches\n",
    "        #print(out.shape)\n",
    "        optimizer.zero_grad() ## initialize the gradients for the new batch\n",
    "        loss = nn.MSELoss()(out, tgt) ## compute the Mean Square Error Loss\n",
    "        loss.backward() ## Backpropagate the loss gradients\n",
    "        optimizer.step() ## update the neural network parameter using the optimizer\n",
    "        epoch_loss.append(loss.item())\n",
    "        #writer.add_scalar(\"error\", loss.item())\n",
    "    print(np.array(np.mean(epoch_loss)))\n",
    "#writer.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}