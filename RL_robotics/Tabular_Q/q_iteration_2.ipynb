{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_iteration_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sgjARee-MgoJ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import collections\n",
        "\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(ENV_NAME)\n",
        "state = env.reset()\n",
        "rewards = collections.defaultdict(float)\n",
        "transits = collections.defaultdict(collections.Counter)\n",
        "values = collections.defaultdict(float)"
      ],
      "metadata": {
        "id": "YXY2eCoUMpfS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_n_random_steps(count):\n",
        "  state = env.reset()\n",
        "  for _ in range(count):\n",
        "      action = env.action_space.sample()\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      rewards[(state, action, new_state)] = reward\n",
        "      transits[(state, action)][new_state] += 1\n",
        "      state = env.reset() if is_done else new_state"
      ],
      "metadata": {
        "id": "j8CXQcnDMwc0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(state):\n",
        "  best_action, best_value = None, None\n",
        "  ## for all the actions an given state find the best action value\n",
        "  for action in range(env.action_space.n):\n",
        "      action_value = values[(state, action)]\n",
        "      if best_value is None or best_value < action_value:\n",
        "          best_value = action_value\n",
        "          best_action = action\n",
        "  return best_action"
      ],
      "metadata": {
        "id": "G8TCpByQM1XO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_episode(env):\n",
        "  total_reward = 0.0\n",
        "  state = env.reset()\n",
        "  while True:\n",
        "      action = select_action(state)\n",
        "      new_state, reward, is_done, _ = env.step(action)\n",
        "      # rewards[(state, action, new_state)] = reward\n",
        "      # transits[(state, action)][new_state] += 1\n",
        "      total_reward += reward\n",
        "      if is_done:\n",
        "          break\n",
        "      state = new_state\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "yKOQjEzVM_Oa"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This is the learning algorithm\n",
        "## the value_iteration function in the Q-Learning case\n",
        "def q_iteration():\n",
        "    ## Find the max(Q(s,a)) for all the states\n",
        "    ## for all the possible states\n",
        "    for state in range(env.observation_space.n):\n",
        "        ## for all the possible actions\n",
        "        for action in range(env.action_space.n):\n",
        "            action_value = 0.0\n",
        "            ## find the transits for a given state action pair\n",
        "            target_counts = transits[(state, action)]\n",
        "            ## total transits for the given state, action pair\n",
        "            total = sum(target_counts.values())\n",
        "            ## For all possible target states find the action value\n",
        "            for tgt_state, count in target_counts.items():\n",
        "                reward = rewards[(state, action, tgt_state)]\n",
        "                ## get the best action value for a given target state \n",
        "                best_action = select_action(tgt_state)\n",
        "                ## self.values[(tgt_state, best_action) is the max_{a`}Q(s`,a`)\n",
        "                action_value += reward + GAMMA * (count / total) * values[(tgt_state, best_action)]\n",
        "            values[(state, action)] = action_value"
      ],
      "metadata": {
        "id": "GJ2TpnKlNG8t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_env = gym.make(ENV_NAME)\n",
        "# writer = SummaryWriter(comment=\"-q-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    iter_no += 1\n",
        "    play_n_random_steps(100)\n",
        "    q_iteration()\n",
        "\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        reward += play_episode(test_env)\n",
        "    reward /= TEST_EPISODES\n",
        "    # writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(\"Solved in %d iterations!\" % iter_no)\n",
        "        break\n",
        "# writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nBoWetNQ3X",
        "outputId": "613d6a4a-042a-4297-fe76-e42633bdcda4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.200\n",
            "Best reward updated 0.200 -> 0.400\n",
            "Best reward updated 0.400 -> 0.450\n",
            "Best reward updated 0.450 -> 0.600\n",
            "Best reward updated 0.600 -> 0.650\n",
            "Best reward updated 0.650 -> 0.800\n",
            "Best reward updated 0.800 -> 0.900\n",
            "Solved in 143 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hHpudO0sNZHs"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}