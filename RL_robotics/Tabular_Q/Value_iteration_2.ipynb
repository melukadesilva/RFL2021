{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Value_iteration_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bzIra1KIafkh"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "ENV_NAME = \"FrozenLake-v0\"\n",
        "\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym \n",
        "\n",
        "env = gym.make(ENV_NAME)\n",
        "rewards = collections.defaultdict(float)\n",
        "transits = collections.defaultdict(collections.Counter)\n",
        "values = collections.defaultdict(float)"
      ],
      "metadata": {
        "id": "26ief69UgNxk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_random_episode(count):\n",
        "  state = env.reset()\n",
        "  for _ in range(count):\n",
        "    # env.render()\n",
        "    # n = env.action_space.n\n",
        "    action = env.action_space.sample()\n",
        "    new_state, reward, is_done, _ = env.step(action)\n",
        "\n",
        "    # print(state)\n",
        "    # print(action)\n",
        "    #print(new_state)\n",
        "\n",
        "    # collect the reward table\n",
        "    rewards[(state, action, new_state)] = reward\n",
        "    transits[(state, action)][new_state] += 1\n",
        "\n",
        "    # set current state to be new_state if the episode is not done\n",
        "    state = env.reset() if is_done else new_state\n",
        "  \n",
        "  # print(rewards)\n",
        "  # print(len(rewards))\n",
        "  ##print(transits)\n",
        "\n",
        "# Test function\n",
        "play_random_episode(3)"
      ],
      "metadata": {
        "id": "O1fq3khwgQwR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the state value for a given action and transition probability using the bellman equation\n",
        "def calculate_state_value(state, action):\n",
        "  # get the transtions using the transition dict\n",
        "  transitions = transits[(state, action)]\n",
        "  total = sum(transitions.values())\n",
        "  state_value = 0.0\n",
        "  # Iterate over the possible transitions and update the state values\n",
        "  for new_state, count in transitions.items():\n",
        "    state_value = state_value + rewards[(state, action, new_state)] + GAMMA * (count / total) * values[new_state]\n",
        "\n",
        "  return state_value"
      ],
      "metadata": {
        "id": "8B4mS8_dyFwS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value iteration; For every state \"s\" find the state value from the bellman optimal equation\n",
        "def value_iteration():\n",
        "  # Iterate for all states\n",
        "  for state in range(env.observation_space.n):\n",
        "    # get state values for all possible actions for a given state\n",
        "    state_values = [calculate_state_value(state, action) for action in range(env.action_space.n)]\n",
        "    # find the action that maximises the state value and update the value table\n",
        "    values[state] = max(state_values)"
      ],
      "metadata": {
        "id": "qrz7I-CAg75c"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the best action given a state by calculating the state values for each actions\n",
        "def select_best_action(state):\n",
        "  best_action, best_value = None, None\n",
        "  for action in range(env.action_space.n):\n",
        "    state_value = calculate_state_value(state, action)\n",
        "    if best_value == None or best_value < state_value:\n",
        "      best_value = state_value\n",
        "      best_action = action\n",
        "\n",
        "  return best_action"
      ],
      "metadata": {
        "id": "qKH80Tgx1QJ0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Play a set of episodes using the value table\n",
        "def play_episode(env):\n",
        "  state = env.reset()\n",
        "  total_reward = 0.0\n",
        "  while True:\n",
        "    # select the best action for a given state from the value table\n",
        "    action = select_best_action(state)\n",
        "    # play the action\n",
        "    new_state, reward, is_done, _ = env.step(action)\n",
        "    # Update the total reward\n",
        "    total_reward += reward\n",
        "    # set next state as current state\n",
        "    state = new_state\n",
        "    # rewards[(state, action, new_state)] = reward\n",
        "    # transits[(state, action)][new_state] += 1\n",
        "    # if episode done; break\n",
        "    if is_done:\n",
        "      break\n",
        "\n",
        "  return total_reward"
      ],
      "metadata": {
        "id": "rMQWmUfc0TL1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Algorithm\n",
        "## 1. Play random episodes and explore the env\n",
        "## 2. Update the value table for the values that maximises the state value\n",
        "## 3. Exploit the env using the best actions and get rewarded\n",
        "## 4. Check if we have reached the reward bound and if so terminate\n",
        "test_env = gym.make(ENV_NAME)\n",
        "# writer = SummaryWriter(comment=\"-v-iteration\")\n",
        "\n",
        "iter_no = 0\n",
        "best_reward = 0.0\n",
        "while True:\n",
        "    #test_env.render()\n",
        "    iter_no += 1\n",
        "    ## Explore the env by playing random actions\n",
        "    ## and update the transit and reward tables\n",
        "    play_random_episode(100)\n",
        "    ## Find the new values of the states after random actions\n",
        "    value_iteration()\n",
        "    #print(values)\n",
        "    #print(transits)\n",
        "    #print(rewards)\n",
        "    reward = 0.0\n",
        "    for _ in range(TEST_EPISODES):\n",
        "        ## Play an episode, this is the exploitation stage\n",
        "        ## in this stage, agent utilises the best actions that maximises the state value\n",
        "        reward += play_episode(test_env)\n",
        "        #print(reward)\n",
        "    reward /= TEST_EPISODES\n",
        "    # writer.add_scalar(\"reward\", reward, iter_no)\n",
        "    if reward > best_reward:\n",
        "        print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
        "        best_reward = reward\n",
        "    if reward > 0.80:\n",
        "        print(\"Solved in %d iterations!\" % iter_no)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuQDHf8E0j8Y",
        "outputId": "0d699d14-ea92-402b-c67d-f83c5c260a08"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best reward updated 0.000 -> 0.150\n",
            "Best reward updated 0.150 -> 0.200\n",
            "Best reward updated 0.200 -> 0.500\n",
            "Best reward updated 0.500 -> 0.650\n",
            "Best reward updated 0.650 -> 0.700\n",
            "Best reward updated 0.700 -> 0.750\n",
            "Best reward updated 0.750 -> 0.900\n",
            "Solved in 179 iterations!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iSUbHCP-0mu-"
      },
      "execution_count": 40,
      "outputs": []
    }
  ]
}